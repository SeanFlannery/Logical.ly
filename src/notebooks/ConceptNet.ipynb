{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "import spacy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize hyper hyper parameters\n",
    "torch.manual_seed(420)\n",
    "np.random.seed(420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://conceptnet.s3.amazonaws.com/downloads/2019/numberbatch/numberbatch-en-19.08.txt.gz\n",
    "#!gunzip numberbatch-en-19.08.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "numberbatch_from_bin = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    '../data/numberbatch-en-19.08.txt', \n",
    "    binary=True, \n",
    "    unicode_errors='ignore'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fallacies_df = pd.read_csv('../data/fallacies.csv', index_col=0)\n",
    "approved_df = pd.read_csv('../data/approved.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([\n",
    "    fallacies_df,\n",
    "    approved_df[approved_df.n_supporters >= 5].drop(\"n_supporters\", axis=1)\n",
    "])\n",
    "df.fallacy_reason = df.fallacy_reason.fillna('')\n",
    "df = df[~df.premise_content.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = df.fallacy_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.fallacy_type.isin(vc.head(10).index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sent):\n",
    "    sent = sent.lower()\n",
    "    sent = nlp(sent)\n",
    "    words = map(lambda x: x.text, sent)\n",
    "    #words = map(lambda x: x.lemma_, sent)\n",
    "    return list(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['premise_content_preprocessed'] = df.premise_content.apply(preprocess_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=420).reset_index(drop=True) # shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vocab = set(chain.from_iterable(map(lambda x: x[1][\"premise_content_preprocessed\"], train_df.iterrows())))\n",
    "sorted_words = sorted(list(word_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip this whole part below if you already have calculated the needed vectors and word map ( we will save and reload them in this notebook, but this could be useful if you were trying to replicate our work and just needed the vectors we have in our ../data directory!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([110, 300])\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {}\n",
    "# we need to create a mapping with all the vectors we might need!\n",
    "needed_vectors = []\n",
    "for i in range(len(sorted_words)):\n",
    "    try:\n",
    "        # Order is important, if we found the word, the we'll append it\n",
    "        # if we haven't we'll consider this an oov word!\n",
    "        needed_vectors.append(numberbatch_from_bin[sorted_words[i]])\n",
    "        word_to_ix[sorted_words[i]] = len(needed_vectors)\n",
    "    except KeyError:\n",
    "        # this will be our OOV term\n",
    "        word_to_ix[sorted_words[i]] = 0\n",
    "\n",
    "\n",
    "#OOV and PAD vectors are both random\n",
    "needed_vectors.insert(0, np.random.randn(*needed_vectors[0].shape))\n",
    "needed_vectors.append(np.random.randn(*needed_vectors[0].shape))\n",
    "word_to_ix['<oov>'] = 0\n",
    "word_to_ix['<pad>'] = len(needed_vectors) - 1\n",
    "\n",
    "#Normalize our vectors\n",
    "needed_vectors = torch.FloatTensor(needed_vectors)\n",
    "norm = needed_vectors.norm(p=2, dim=1, keepdim=True)\n",
    "needed_vectors = needed_vectors.div(norm)\n",
    "\n",
    "torch.save(needed_vectors,'../data/numberbatch_needed_vectors.pt')\n",
    "# Also make sure to save the word_map\n",
    "print(needed_vectors.shape)\n",
    "f = open(\"../data/numberbatch_word_to_ix.pkl\", \"wb\")\n",
    "pickle.dump(word_to_ix, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "needed_vectors = torch.load('../data/numberbatch_needed_vectors.pt')\n",
    "with open(\"../data/numberbatch_word_to_ix.pkl\", 'rb') as fr:\n",
    "    word_map = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "fallacy_vocab = sorted(list(set(df.fallacy_type.unique())))\n",
    "fallacy_to_ix = {word: i for i, word in enumerate(fallacy_vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, word_vocab_size, word_embedding_dim, fallacy_vocab_size, max_sent_size, numberbatch_tensor):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(numberbatch_tensor)\n",
    "\n",
    "        hidden = 100\n",
    "        self.fc1 = nn.Linear(word_embedding_dim, hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden * max_sent_size, fallacy_vocab_size)\n",
    "        \n",
    "    def forward(self, word_inputs):\n",
    "        word_embeds = self.word_embeddings(word_inputs)\n",
    "        h1 = self.fc1(word_embeds)\n",
    "        a1 = self.relu(h1)\n",
    "        h2 = self.fc2(a1.view(a1.shape[0], -1))\n",
    "        return h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_and_convert_to_ints(data):\n",
    "    X = np.full((len(data), MAX_SENT_SIZE), word_to_ix[\"<pad>\"])\n",
    "\n",
    "    for i, (_, x) in enumerate(data.iterrows()):\n",
    "        X[i, :len(x[\"premise_content_preprocessed\"])] = [\n",
    "            (word_to_ix[word] if word in word_to_ix else word_to_ix['<oov>'])\n",
    "            for word in x[\"premise_content_preprocessed\"]\n",
    "        ]\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = pad_and_convert_to_ints(train_df)\n",
    "testX = pad_and_convert_to_ints(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY = train_df.fallacy_type.apply(lambda x: fallacy_to_ix[x]).values\n",
    "testY = test_df.fallacy_type.apply(lambda x: fallacy_to_ix[x]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(\n",
    "    len(word_vocab),\n",
    "    300,\n",
    "    len(fallacy_vocab),\n",
    "    MAX_SENT_SIZE,\n",
    "    needed_vectors\n",
    ")\n",
    "opt = optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(X, Y, model, opt, criterion, batch_size=50):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for beg_i in range(0, X.shape[0], batch_size):\n",
    "        x_batch = X[beg_i : beg_i + batch_size, :]\n",
    "        y_batch = Y[beg_i : beg_i + batch_size]\n",
    "        x_batch = torch.tensor(x_batch)\n",
    "        y_batch = torch.tensor(y_batch)\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        y_pred = model(x_batch)\n",
    "\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "\n",
    "        losses.append(loss.data.numpy())\n",
    "\n",
    "    return [sum(losses) / float(len(losses))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:15<00:00,  3.49it/s]\n"
     ]
    }
   ],
   "source": [
    "e_losses = []\n",
    "num_epochs = 50\n",
    "for e in tqdm(range(num_epochs)):\n",
    "    e_losses += train_epoch(trainX, trainY, net, opt, criterion, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.6239029659944422,\n",
       " 1.4320020745782291,\n",
       " 1.3878648491466747,\n",
       " 1.3392051037620096,\n",
       " 1.2825453912510592,\n",
       " 1.2250224772621603,\n",
       " 1.1703543417594011,\n",
       " 1.1202354396090788,\n",
       " 1.078411407330457,\n",
       " 1.0414888280279495,\n",
       " 1.0083690986913794,\n",
       " 0.9761710359769709,\n",
       " 0.9445623229531681,\n",
       " 0.9234212373985964,\n",
       " 0.898720977937474,\n",
       " 0.8773404500063728,\n",
       " 0.8553482381736531,\n",
       " 0.8343695647576276,\n",
       " 0.8156078142278335,\n",
       " 0.7992306001046124,\n",
       " 0.7831716274513918,\n",
       " 0.769335678395103,\n",
       " 0.7552255129112917,\n",
       " 0.74031402258312,\n",
       " 0.7267584818250993,\n",
       " 0.7142733002410215,\n",
       " 0.7066754295545465,\n",
       " 0.6931656967191135,\n",
       " 0.6847570205436033,\n",
       " 0.6752560436725616,\n",
       " 0.6656898242585799,\n",
       " 0.6573495952522054,\n",
       " 0.6491793236311745,\n",
       " 0.6416953942354988,\n",
       " 0.6349883658044478,\n",
       " 0.6279487574801725,\n",
       " 0.6213392545195187,\n",
       " 0.6162620993221507,\n",
       " 0.6106918278862449,\n",
       " 0.6053871126735911,\n",
       " 0.6002449042656842,\n",
       " 0.5942175195497625,\n",
       " 0.585710932226742,\n",
       " 0.5816615784869474,\n",
       " 0.5749589949846268,\n",
       " 0.5699315947644851,\n",
       " 0.5587790801244623,\n",
       " 0.5572669199284386,\n",
       " 0.5517931264989516,\n",
       " 0.547615177491132]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics (Macro F1 Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    x = torch.tensor(testX)\n",
    "    y_pred = net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "score = f1_score(testY, y_pred.numpy().argmax(axis=1), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13829441974882395"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guessing Appeal To Authority\t for all:\t0.002442002442002442\n",
      "Guessing Appeal To Belief\t for all:\t0.007168458781362007\n",
      "Guessing Begging The Question\t for all:\t0.010582010582010581\n",
      "Guessing Fallacy Of False Cause\t for all:\t0.007168458781362007\n",
      "Guessing Fallacy Of Red Herring\t for all:\t0.012798138452588714\n",
      "Guessing Irrelevant Conclusion\t for all:\t0.03442879499217527\n",
      "Guessing None\t for all:\t0.081377151799687\n",
      "Guessing Poisoning The Well\t for all:\t0.002442002442002442\n",
      "Guessing Prejudicial Language\t for all:\t0.0\n",
      "Guessing Wrong Direction\t for all:\t0.008318478906714201\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\"Guessing \" + fallacy_vocab[i] + \"\\t for all:\", end='\\t')\n",
    "    print(f1_score(testY, np.array( [i]*testY.shape[0]), average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Guessing:\t0.07239885972537836\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Guessing:\", end='\\t')\n",
    "print(f1_score(testY, np.random.randint(0,10,testY.shape[0]), average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
