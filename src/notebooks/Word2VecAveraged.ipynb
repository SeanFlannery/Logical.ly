{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elnardu/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "import spacy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize hyper hyper parameters\n",
    "torch.manual_seed(420)\n",
    "np.random.seed(420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE -- You just need to provide your own path to a w2v binary here if you don't have it\n",
    "# we used the one from our NLP Course's 2nd HW Assignment\n",
    "wv_from_bin = gensim.models.KeyedVectors.load_word2vec_format(\"../data/w2v.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fallacies_df = pd.read_csv('../data/fallacies.csv', index_col=0)\n",
    "approved_df = pd.read_csv('../data/approved.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([\n",
    "    fallacies_df,\n",
    "    approved_df[approved_df.n_supporters >= 5].drop(\"n_supporters\", axis=1)\n",
    "])\n",
    "df.fallacy_reason = df.fallacy_reason.fillna('')\n",
    "df = df[~df.premise_content.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = df.fallacy_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.fallacy_type.isin(vc.head(10).index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0509 22:31:43.230842 4594539968 file_utils.py:39] PyTorch version 1.0.1.post2 available.\n",
      "I0509 22:31:43.267592 4594539968 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sent):\n",
    "    sent = sent.lower()\n",
    "    sent = nlp(sent)\n",
    "    words = map(lambda x: x.text, sent)\n",
    "    return list(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['premise_content_preprocessed'] = df.premise_content.apply(preprocess_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=420).reset_index(drop=True) # shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vocab = set(chain.from_iterable(map(lambda x: x[1][\"premise_content_preprocessed\"], train_df.iterrows())))\n",
    "sorted_words = sorted(list(word_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip this whole part below if you already have calculated the needed vectors and word map ( we will save and reload them in this notebook, but this could be useful if you were trying to replicate our work and just needed the vectors we have in our ../data directory!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {}\n",
    "# we need to create a mapping with all the vectors we might need!\n",
    "needed_vectors = []\n",
    "for i in range(len(sorted_words)):\n",
    "    try:\n",
    "        # Order is important, if we found the word, the we'll append it\n",
    "        # if we haven't we'll consider this an oov word!\n",
    "        needed_vectors.append(wv_from_bin[sorted_words[i]])\n",
    "        word_to_ix[sorted_words[i]] = len(needed_vectors)\n",
    "    except KeyError:\n",
    "        # this will be our OOV term\n",
    "        word_to_ix[sorted_words[i]] = 0\n",
    "\n",
    "\n",
    "#OOV and PAD vectors are both random\n",
    "needed_vectors.insert(0, np.random.randn(*needed_vectors[0].shape))\n",
    "needed_vectors.append(np.random.randn(*needed_vectors[0].shape))\n",
    "word_to_ix['<oov>'] = 0\n",
    "word_to_ix['<pad>'] = len(needed_vectors) - 1\n",
    "\n",
    "needed_vectors = torch.FloatTensor(needed_vectors)\n",
    "torch.save(needed_vectors,'../data/w2v_needed_vectors.pt')\n",
    "# Also make sure to save the word_map\n",
    "print(needed_vectors.shape)\n",
    "f = open(\"../data/w2v_word_to_ix.pkl\", \"wb\")\n",
    "pickle.dump(word_to_ix, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "needed_vectors = torch.load('../data/w2v_needed_vectors.pt')\n",
    "with open(\"../data/w2v_word_to_ix.pkl\", 'rb') as fr:\n",
    "    word_to_ix = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fallacy_vocab = sorted(list(set(df.fallacy_type.unique())))\n",
    "fallacy_to_ix = {word: i for i, word in enumerate(fallacy_vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, word_vocab_size, word_embedding_dim, fallacy_vocab_size, max_sent_size, word2vec_tensor):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(word2vec_tensor)\n",
    "\n",
    "        hidden = 100\n",
    "        self.fc1 = nn.Linear(word_embedding_dim, hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden, fallacy_vocab_size)\n",
    "        \n",
    "    def forward(self, word_inputs):\n",
    "        word_embeds = self.word_embeddings(word_inputs)\n",
    "        word_embeds_avg = word_embeds.sum(dim=1) / word_embeds.shape[2]\n",
    "        h1 = self.fc1(word_embeds_avg)\n",
    "        a1 = self.relu(h1)\n",
    "        h2 = self.fc2(a1.view(a1.shape[0], -1))\n",
    "        return h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_and_convert_to_ints(data):\n",
    "    X = np.full((len(data), MAX_SENT_SIZE), word_to_ix[\"<pad>\"])\n",
    "\n",
    "    for i, (_, x) in enumerate(data.iterrows()):\n",
    "        X[i, :len(x[\"premise_content_preprocessed\"])] = [\n",
    "            (word_to_ix[word] if word in word_to_ix else word_to_ix['<oov>'])\n",
    "            for word in x[\"premise_content_preprocessed\"]\n",
    "        ]\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = pad_and_convert_to_ints(train_df)\n",
    "testX = pad_and_convert_to_ints(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY = train_df.fallacy_type.apply(lambda x: fallacy_to_ix[x]).values\n",
    "testY = test_df.fallacy_type.apply(lambda x: fallacy_to_ix[x]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(\n",
    "    len(word_vocab),\n",
    "    300,\n",
    "    len(fallacy_vocab),\n",
    "    MAX_SENT_SIZE,\n",
    "    needed_vectors\n",
    ")\n",
    "opt = optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(X, Y, model, opt, criterion, batch_size=50):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for beg_i in range(0, X.shape[0], batch_size):\n",
    "        x_batch = X[beg_i : beg_i + batch_size, :]\n",
    "        y_batch = Y[beg_i : beg_i + batch_size]\n",
    "        x_batch = torch.tensor(x_batch)\n",
    "        y_batch = torch.tensor(y_batch)\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        y_pred = model(x_batch)\n",
    "\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "\n",
    "        losses.append(loss.data.numpy())\n",
    "\n",
    "    return [sum(losses) / float(len(losses))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 14.12it/s]\n"
     ]
    }
   ],
   "source": [
    "e_losses = []\n",
    "num_epochs = 50\n",
    "for e in tqdm(range(num_epochs)):\n",
    "    e_losses += train_epoch(trainX, trainY, net, opt, criterion, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.4345142911462223,\n",
       " 1.4344056073357077,\n",
       " 1.434269856004154,\n",
       " 1.4341146735584034,\n",
       " 1.4339951767640955,\n",
       " 1.4338882530436796,\n",
       " 1.433789141037885,\n",
       " 1.4336570781819962,\n",
       " 1.4335425180547379,\n",
       " 1.4334451661390417,\n",
       " 1.4333310898612528,\n",
       " 1.4332070350646973,\n",
       " 1.4331041644601261,\n",
       " 1.4329929421929752,\n",
       " 1.4329091520870434,\n",
       " 1.4327978316475363,\n",
       " 1.4326749829684986,\n",
       " 1.4325711867388558,\n",
       " 1.4324599504470825,\n",
       " 1.4323640500797945,\n",
       " 1.43224914634929,\n",
       " 1.4321734764996696,\n",
       " 1.4320571142084457,\n",
       " 1.4319381994359635,\n",
       " 1.4318555383121265,\n",
       " 1.4317386921714335,\n",
       " 1.4316397975472843,\n",
       " 1.4315261139589197,\n",
       " 1.4314312934875488,\n",
       " 1.4313431347117704,\n",
       " 1.4312334130792057,\n",
       " 1.4311270713806152,\n",
       " 1.4310133948045618,\n",
       " 1.4309110992095049,\n",
       " 1.4308180738897884,\n",
       " 1.4307219841900993,\n",
       " 1.4306006291333366,\n",
       " 1.4305181643542122,\n",
       " 1.4303987376830156,\n",
       " 1.43030443612267,\n",
       " 1.430184757008272,\n",
       " 1.430087874917423,\n",
       " 1.4299801868550919,\n",
       " 1.4298759208006018,\n",
       " 1.429769193424898,\n",
       " 1.4296819041757023,\n",
       " 1.4295689498676973,\n",
       " 1.429430596968707,\n",
       " 1.4293333782869226,\n",
       " 1.4292267420712639]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics (Macro F1 Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    x = torch.tensor(testX)\n",
    "    y_pred = net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elnardu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "score = f1_score(testY, y_pred.numpy().argmax(axis=1), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07959866220735787"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guessing Appeal To Authority\t for all:\t0.004347826086956523\n",
      "Guessing Appeal To Belief\t for all:\t0.0064516129032258064\n",
      "Guessing Begging The Question\t for all:\t0.005405405405405406\n",
      "Guessing Fallacy Of False Cause\t for all:\t0.00851063829787234\n",
      "Guessing Fallacy Of Red Herring\t for all:\t0.005405405405405406\n",
      "Guessing Irrelevant Conclusion\t for all:\t0.02178217821782178\n",
      "Guessing None\t for all:\t0.07959866220735787\n",
      "Guessing Poisoning The Well\t for all:\t0.0011049723756906078\n",
      "Guessing Prejudicial Language\t for all:\t0.003278688524590164\n",
      "Guessing Wrong Direction\t for all:\t0.0074866310160427805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elnardu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/elnardu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/elnardu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/elnardu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/elnardu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/elnardu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/elnardu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/elnardu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/elnardu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/elnardu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\"Guessing \" + fallacy_vocab[i] + \"\\t for all:\", end='\\t')\n",
    "    print(f1_score(testY, np.array( [i]*testY.shape[0]), average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Guessing:\t0.02837729816147082\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Guessing:\", end='\\t')\n",
    "print(f1_score(testY, np.random.randint(0,10,testY.shape[0]), average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
