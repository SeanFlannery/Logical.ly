{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elnardu/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "import spacy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize hyper hyper parameters\n",
    "torch.manual_seed(420)\n",
    "np.random.seed(420)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fallacies_df = pd.read_csv('../data/fallacies.csv', index_col=0)\n",
    "approved_df = pd.read_csv('../data/approved.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([\n",
    "    fallacies_df,\n",
    "    approved_df[approved_df.n_supporters >= 5].drop(\"n_supporters\", axis=1)\n",
    "])\n",
    "df.fallacy_reason = df.fallacy_reason.fillna('')\n",
    "df = df[~df.premise_content.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = df.fallacy_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.fallacy_type.isin(vc.head(10).index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sent):\n",
    "    sent = sent.lower()\n",
    "    sent = nlp(sent)\n",
    "    words = map(lambda x: x.text, sent)\n",
    "    return list(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['premise_content_preprocessed'] = df.premise_content.apply(preprocess_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True) # shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vocab = {\"<oov>\", \"<pad>\"}\n",
    "word_vocab = word_vocab.union(\n",
    "    set(chain.from_iterable(map(lambda x: x[1][\"premise_content_preprocessed\"], train_df.iterrows())))\n",
    ")\n",
    "word_to_ix = {word: i for i, word in enumerate(word_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fallacy_vocab = set(df.fallacy_type.unique())\n",
    "fallacy_to_ix = {word: i for i, word in enumerate(fallacy_vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, word_vocab_size, word_embedding_dim, fallacy_vocab_size, max_sent_size):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(\n",
    "            word_vocab_size, word_embedding_dim\n",
    "        )  # random init\n",
    "\n",
    "        hidden = 100\n",
    "        self.fc1 = nn.Linear(word_embedding_dim, hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden * max_sent_size, fallacy_vocab_size)\n",
    "        \n",
    "    def forward(self, word_inputs):\n",
    "        word_embeds = self.word_embeddings(word_inputs)\n",
    "        h1 = self.fc1(word_embeds)\n",
    "        a1 = self.relu(h1)\n",
    "        h2 = self.fc2(a1.view(a1.shape[0], -1))\n",
    "        return h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_and_convert_to_ints(data):\n",
    "    X = np.full((len(data), MAX_SENT_SIZE), word_to_ix[\"<pad>\"])\n",
    "\n",
    "    for i, (_, x) in enumerate(data.iterrows()):\n",
    "        X[i, :len(x[\"premise_content_preprocessed\"])] = [\n",
    "            (word_to_ix[word] if word in word_to_ix else word_to_ix['<oov>'])\n",
    "            for word in x[\"premise_content_preprocessed\"]\n",
    "        ]\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = pad_and_convert_to_ints(train_df)\n",
    "testX = pad_and_convert_to_ints(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY = train_df.fallacy_type.apply(lambda x: fallacy_to_ix[x]).values\n",
    "testY = test_df.fallacy_type.apply(lambda x: fallacy_to_ix[x]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(\n",
    "    len(word_vocab),\n",
    "    100,\n",
    "    len(fallacy_vocab),\n",
    "    MAX_SENT_SIZE,\n",
    ")\n",
    "opt = optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(X, Y, model, opt, criterion, batch_size=50):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for beg_i in range(0, X.shape[0], batch_size):\n",
    "        x_batch = X[beg_i : beg_i + batch_size, :]\n",
    "        y_batch = Y[beg_i : beg_i + batch_size]\n",
    "        x_batch = torch.tensor(x_batch)\n",
    "        y_batch = torch.tensor(y_batch)\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        y_pred = model(x_batch)\n",
    "\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "\n",
    "        losses.append(loss.data.numpy())\n",
    "\n",
    "    return [sum(losses) / float(len(losses))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:49<00:00,  1.01it/s]\n"
     ]
    }
   ],
   "source": [
    "e_losses = []\n",
    "num_epochs = 50\n",
    "for e in tqdm(range(num_epochs)):\n",
    "    e_losses += train_epoch(trainX, trainY, net, opt, criterion, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.648753839380601,\n",
       " 1.21101713180542,\n",
       " 0.9515892758088953,\n",
       " 0.7630115361774669,\n",
       " 0.6125645865412319,\n",
       " 0.4951530668665381,\n",
       " 0.408632590490229,\n",
       " 0.3428669269470608,\n",
       " 0.2935536319718641,\n",
       " 0.2552408746936742,\n",
       " 0.22508155182003975,\n",
       " 0.20155408912721803,\n",
       " 0.1859373873209252,\n",
       " 0.16836540346198223,\n",
       " 0.15941342382746584,\n",
       " 0.14679257144384525,\n",
       " 0.1432666515602785,\n",
       " 0.13273896019467535,\n",
       " 0.1333230619921404,\n",
       " 0.12349908179877435,\n",
       " 0.12633169113713152,\n",
       " 0.11653816256233875,\n",
       " 0.12042990858283113,\n",
       " 0.11153627923854134,\n",
       " 0.11452598494532354,\n",
       " 0.10918897406800705,\n",
       " 0.10946088349994491,\n",
       " 0.10514081118847518,\n",
       " 0.10541462008019581,\n",
       " 0.10281893854741664,\n",
       " 0.10362636391073465,\n",
       " 0.10126961147248306,\n",
       " 0.10245951656799983,\n",
       " 0.1000685046070858,\n",
       " 0.10098900229615324,\n",
       " 0.09892782963374082,\n",
       " 0.10043832237887032,\n",
       " 0.0977024675675613,\n",
       " 0.09950917482595234,\n",
       " 0.0964931370690465,\n",
       " 0.09885691539110507,\n",
       " 0.09524452180930358,\n",
       " 0.09799717498176239,\n",
       " 0.09421895990860374,\n",
       " 0.09737113296218655,\n",
       " 0.09327493162880487,\n",
       " 0.09671913172282717,\n",
       " 0.09249564056175158,\n",
       " 0.09615905500729294,\n",
       " 0.09176802861175555]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    x = torch.tensor(testX)\n",
    "    y_pred = net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = f1_score(testY, y_pred.numpy().argmax(axis=1), average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6611111111111111"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
