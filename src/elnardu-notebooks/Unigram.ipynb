{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import spacy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "fallacies_df = pd.read_csv('../data/fallacies.csv', index_col=0)\n",
    "approved_df = pd.read_csv('../data/approved.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.fallacy_reason = df.fallacy_reason.fillna('')\n",
    "\n",
    "df = pd.concat([\n",
    "    fallacies_df,\n",
    "    approved_df[approved_df.n_supporters >= 5].drop(\"n_supporters\", axis=1)\n",
    "])\n",
    "\n",
    "df = df[~df.premise_content.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = df.fallacy_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.fallacy_type.isin(vc.head(10).index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sent):\n",
    "    sent = sent.lower()\n",
    "    sent = nlp(sent)\n",
    "    words = map(lambda x: x.text, sent)\n",
    "    return list(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['premise_content_preprocessed'] = df.premise_content.apply(preprocess_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True) # shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vocab = {\"<oov>\", \"<pad>\"}\n",
    "word_vocab = word_vocab.union(\n",
    "    set(chain.from_iterable(map(lambda x: x[1][\"premise_content_preprocessed\"], train_df.iterrows())))\n",
    ")\n",
    "word_to_ix = {word: i for i, word in enumerate(word_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "fallacy_vocab = set(df.fallacy_type.unique())\n",
    "fallacy_to_ix = {word: i for i, word in enumerate(fallacy_vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, word_vocab_size, word_embedding_dim):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(\n",
    "            word_vocab_size, word_embedding_dim\n",
    "        )  # random init\n",
    "\n",
    "        hidden = 100\n",
    "        self.fc1 = nn.Linear(word_embedding_dim, hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden, 1)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=2)\n",
    "        \n",
    "    def forward(self, word_inputs):\n",
    "        word_embeds = self.word_embeddings(word_inputs)\n",
    "        h1 = self.fc1(word_embeds)\n",
    "        a1 = self.relu(h1)\n",
    "        h2 = self.fc2(a1)\n",
    "        return h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_and_convert_to_ints(data):\n",
    "    max_size = len(\n",
    "        max(data.iterrows(),\n",
    "            key=lambda x: len(x[1][\"premise_content_preprocessed\"]))[1]\n",
    "        [\"premise_content_preprocessed\"])\n",
    "    X = np.full((len(data), max_size), word_to_ix[\"<pad>\"])\n",
    "\n",
    "    for i, (_, x) in enumerate(data.iterrows()):\n",
    "        X[i, :len(x[\"premise_content_preprocessed\"])] = [\n",
    "            (word_to_ix[word] if word in word_to_ix else word_to_ix['<oov>'])\n",
    "            for word in x[\"premise_content_preprocessed\"]\n",
    "        ]\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = pad_and_convert_to_ints(train_df)\n",
    "testX = pad_and_convert_to_ints(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY = train_df.fallacy_type.apply(lambda x: fallacy_to_ix[x]).values.reshape(-1, 1)\n",
    "testY = test_df.fallacy_type.apply(lambda x: fallacy_to_ix[x]).values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(\n",
    "    len(word_vocab),\n",
    "    10,\n",
    ")\n",
    "opt = optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(X, Y, model, opt, criterion, batch_size=50):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for beg_i in range(0, X.shape[0], batch_size):\n",
    "        x_batch = X[beg_i : beg_i + batch_size, :]\n",
    "        y_batch = Y[beg_i : beg_i + batch_size, :]\n",
    "        x_batch = torch.tensor(x_batch)\n",
    "        y_batch = torch.tensor(y_batch)\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        y_pred = model(x_batch)\n",
    "\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "\n",
    "        losses.append(loss.data.numpy())\n",
    "\n",
    "    return [sum(losses) / float(len(losses))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:57<00:00,  1.75it/s]\n"
     ]
    }
   ],
   "source": [
    "e_losses = []\n",
    "num_epochs = 100\n",
    "for e in tqdm(range(num_epochs)):\n",
    "    e_losses += train_epoch(trainX, trainY, net, opt, criterion, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.0380179741803337,\n",
       " 3.0063493672539208,\n",
       " 2.9753541525672462,\n",
       " 2.944806505652035,\n",
       " 2.915020185358384,\n",
       " 2.885999258826761,\n",
       " 2.857938892701093,\n",
       " 2.8311156525331387,\n",
       " 2.8055960991803337,\n",
       " 2.781282424926758,\n",
       " 2.7582375722772934,\n",
       " 2.736333173864028,\n",
       " 2.715488686281092,\n",
       " 2.6957156517926384,\n",
       " 2.6770255004658416,\n",
       " 2.659271899391623,\n",
       " 2.642465717652265,\n",
       " 2.626415224636302,\n",
       " 2.611290693283081,\n",
       " 2.5968673088971306,\n",
       " 2.5831929936128506,\n",
       " 2.570176699582268,\n",
       " 2.557802480809829,\n",
       " 2.5459998775930965,\n",
       " 2.5346695395076977,\n",
       " 2.5238923605750587,\n",
       " 2.5136453965130974,\n",
       " 2.503760113435633,\n",
       " 2.494337095933802,\n",
       " 2.485317650963278,\n",
       " 2.476711637833539,\n",
       " 2.4684326929204605,\n",
       " 2.4605423422420727,\n",
       " 2.4529862544115852,\n",
       " 2.445765130660113,\n",
       " 2.4388511461370133,\n",
       " 2.432255282121546,\n",
       " 2.4258906140046963,\n",
       " 2.419803437064676,\n",
       " 2.4139573714312386,\n",
       " 2.408359681858736,\n",
       " 2.4029767934013817,\n",
       " 2.3978470353519215,\n",
       " 2.3928763024947224,\n",
       " 2.3881373405456543,\n",
       " 2.383571554632748,\n",
       " 2.379203614066629,\n",
       " 2.374945233849918,\n",
       " 2.3708842642167034,\n",
       " 2.3669783788568832,\n",
       " 2.3631720963646385,\n",
       " 2.3595573271022126,\n",
       " 2.3560794451657463,\n",
       " 2.3527179886313045,\n",
       " 2.3495388872483196,\n",
       " 2.3464058847988354,\n",
       " 2.3434433937072754,\n",
       " 2.340582679299747,\n",
       " 2.3378022418302646,\n",
       " 2.3351361330817726,\n",
       " 2.3325825508903053,\n",
       " 2.3301143996855793,\n",
       " 2.327743221731747,\n",
       " 2.3254563878564274,\n",
       " 2.3232440247255215,\n",
       " 2.321110493996564,\n",
       " 2.319061012829051,\n",
       " 2.317060309297898,\n",
       " 2.315166010576136,\n",
       " 2.3132916057811066,\n",
       " 2.3114912159302654,\n",
       " 2.3097202637616325,\n",
       " 2.308040331391727,\n",
       " 2.306402760393479,\n",
       " 2.3048200677422916,\n",
       " 2.3032967062557446,\n",
       " 2.301800748881172,\n",
       " 2.3003956079483032,\n",
       " 2.2990149049197925,\n",
       " 2.297675784896402,\n",
       " 2.2963945444892433,\n",
       " 2.2951137879315544,\n",
       " 2.2939107207690967,\n",
       " 2.2927286484662224,\n",
       " 2.2915833557353302,\n",
       " 2.2904991262099323,\n",
       " 2.289426193517797,\n",
       " 2.28841111239265,\n",
       " 2.287392426939572,\n",
       " 2.286467341815724,\n",
       " 2.2854903375401214,\n",
       " 2.284606561941259,\n",
       " 2.283741740619435,\n",
       " 2.282893482376547,\n",
       " 2.282072438913233,\n",
       " 2.2812638633391438,\n",
       " 2.2804895569296444,\n",
       " 2.2797514691072354,\n",
       " 2.2790094754275154,\n",
       " 2.2783393509247722]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
